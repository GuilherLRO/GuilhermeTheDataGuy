{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa33b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg\n",
      "  Using cached ffmpeg-1.4-py3-none-any.whl\n",
      "Installing collected packages: ffmpeg\n",
      "Successfully installed ffmpeg-1.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b2302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Using cached openai_whisper-20250625-py3-none-any.whl\n",
      "Collecting more-itertools (from openai-whisper)\n",
      "  Using cached more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting numba (from openai-whisper)\n",
      "  Downloading numba-0.62.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in /Users/guilherme/Documents/GitHub/small_projects/.venv/lib/python3.12/site-packages (from openai-whisper) (2.3.1)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Using cached tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting torch (from openai-whisper)\n",
      "  Using cached torch-2.8.0-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: tqdm in /Users/guilherme/Documents/GitHub/small_projects/.venv/lib/python3.12/site-packages (from openai-whisper) (4.67.1)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba->openai-whisper)\n",
      "  Downloading llvmlite-0.45.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->openai-whisper)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/guilherme/Documents/GitHub/small_projects/.venv/lib/python3.12/site-packages (from tiktoken->openai-whisper) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/guilherme/Documents/GitHub/small_projects/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/guilherme/Documents/GitHub/small_projects/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/guilherme/Documents/GitHub/small_projects/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/guilherme/Documents/GitHub/small_projects/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.7.9)\n",
      "Collecting filelock (from torch->openai-whisper)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch->openai-whisper)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting setuptools (from torch->openai-whisper)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch->openai-whisper)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->openai-whisper)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch->openai-whisper)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch->openai-whisper)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->openai-whisper)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->openai-whisper)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Using cached more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "Downloading numba-0.62.0-cp312-cp312-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading llvmlite-0.45.0-cp312-cp312-macosx_11_0_arm64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl (996 kB)\n",
      "Downloading regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl (287 kB)\n",
      "Using cached torch-2.8.0-cp312-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, regex, networkx, more-itertools, MarkupSafe, llvmlite, fsspec, filelock, tiktoken, numba, jinja2, torch, openai-whisper\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [openai-whisper]m [torch]te]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 llvmlite-0.45.0 more-itertools-10.8.0 mpmath-1.3.0 networkx-3.5 numba-0.62.0 openai-whisper-20250625 regex-2025.9.18 setuptools-80.9.0 sympy-1.14.0 tiktoken-0.11.0 torch-2.8.0 typing-extensions-4.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80363e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in /Users/guilherme/Documents/GitHub/small_projects/.venv/lib/python3.12/site-packages (2025.7.9)\n",
      "Collecting certifi\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: certifi\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.7.9\n",
      "    Uninstalling certifi-2025.7.9:\n",
      "      Successfully uninstalled certifi-2025.7.9\n",
      "Successfully installed certifi-2025.8.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f835fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e11973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Model loaded.\n",
      "Starting transcription of 'ben_mentoring_august.mov'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilherme/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:07.120]  Old work, it's very much me getting off it is dependent on these other migrations that are going on,\n",
      "[00:07.120 --> 00:12.080]  kind of rebuilding systems, this new team taking over, which that just keeps getting delayed.\n",
      "[00:12.080 --> 00:16.720]  Now it's looking like the targets for everything are like end of September, that could get pushed back even more.\n",
      "[00:16.720 --> 00:19.640]  So I'm definitely still kind of like one foot over there.\n",
      "[00:19.640 --> 00:24.960]  But with the new work, I've been getting a lot more involved, doing a lot more engineering work on the signal side.\n",
      "[00:24.960 --> 00:29.240]  So that's been really exciting. I haven't been able to fully commit to it because of capacity.\n",
      "[00:29.280 --> 00:34.840]  But for example, last week I was able to put a lot of time into it. I'm hoping this week I can put in some more.\n",
      "[00:34.840 --> 00:38.960]  So it's a bit of getting stretched between two different teams and two different roles.\n",
      "[00:38.960 --> 00:43.080]  But overall, it's been fine.\n",
      "[00:43.080 --> 00:47.520]  And yeah, very eager to kind of start the new work.\n",
      "[00:47.520 --> 00:51.600]  Just like a lot of kind of moving parts and know it can be a bit tough sometimes when it's out of your control.\n",
      "[00:51.600 --> 00:58.280]  You know, it's very much relying on these other teams kind of making progress on their migrations and everything.\n",
      "[00:58.280 --> 01:03.120]  So yeah, so that's about it. Otherwise, it's been kind of like more of the same, I'd say, since the last time we spoke.\n",
      "[01:04.920 --> 01:09.560]  But yeah, how about you? You said things are unusually busy right now.\n",
      "[01:10.200 --> 01:15.040]  Oh, yeah. Let me I don't I didn't even know why there are so many.\n",
      "[01:15.040 --> 01:22.560]  I have like one, two, three. I usually don't have a lot of meetings, but they just keep they keep happening.\n",
      "[01:22.560 --> 01:25.600]  You know, Nike is going to be our client.\n",
      "[01:25.640 --> 01:29.200]  And that by itself is a really big thing.\n",
      "[01:30.440 --> 01:35.520]  There's a lot of back and forth about like the contract they haven't signed like they.\n",
      "[01:36.440 --> 01:38.960]  Yeah, there's a lot of like small thing.\n",
      "[01:38.960 --> 01:43.960]  I Rob mentioned that they've been talking with Nike since like 2019.\n",
      "[01:45.240 --> 01:50.320]  And we met them like I met them, the people from there, like for the first time yesterday.\n",
      "[01:50.880 --> 01:56.120]  And it's just a lot of things they want to release, like specific things.\n",
      "[01:56.600 --> 02:05.640]  And we are actually going to start working with apparel data because we we only sold this data to GDMAX.\n",
      "[02:06.240 --> 02:11.040]  And that was something that I that I kind of did by myself like a year ago.\n",
      "[02:11.040 --> 02:13.160]  I started like mapping those items.\n",
      "[02:13.160 --> 02:20.240]  And it's really exciting because as we they're kind of skeptical on their side about what we can do with their data.\n",
      "[02:21.200 --> 02:27.080]  Let's say and probably we live this partnership with Nike is it works.\n",
      "[02:27.400 --> 02:33.000]  Eventually, we may be able to sell to other players like Adidas or, you know, anyone else.\n",
      "[02:33.680 --> 02:37.840]  Yeah, well, but there is a lot that we have to learn yet.\n",
      "[02:39.160 --> 02:43.880]  It's really new. And they want to see things in a very specific way.\n",
      "[02:43.880 --> 02:47.120]  That's why they they have a very big contract with numerator.\n",
      "[02:48.120 --> 02:57.800]  But the thing they're going after that they look they're looking for is the way to as someone that can aggregate the data in the specific way they do that.\n",
      "[02:57.800 --> 02:58.800]  That's very unique.\n",
      "[03:00.080 --> 03:02.120]  So, yeah, we have to start working on it.\n",
      "[03:02.120 --> 03:04.120]  And I'm starting with that.\n",
      "[03:04.120 --> 03:07.760]  But, you know, they take time and a lot of meetings are happening.\n",
      "[03:08.400 --> 03:11.280]  Yeah. At the same time, there are still transitions.\n",
      "[03:11.520 --> 03:24.360]  There are like still clients that are support from the, you know, that they were previously mine and we started transitioning them like right now as we restructure the corporate team.\n",
      "[03:27.600 --> 03:31.120]  Yeah, basically that it also has to support.\n",
      "[03:31.320 --> 03:33.320]  I'm supporting beauty.\n",
      "[03:33.320 --> 03:40.560]  So and a lot of like last minute changes that need to happen, for example.\n",
      "[03:41.360 --> 03:47.120]  I know we're representing somewhat something to someone and we found out that those items are misclassified.\n",
      "[03:48.160 --> 03:52.360]  Also, there are problems with the data science tools.\n",
      "[03:52.360 --> 03:56.240]  I've been talking a lot of the data science and data platform team.\n",
      "[03:56.240 --> 04:00.000]  So a lot a lot has been happening lately.\n",
      "[04:00.600 --> 04:03.000]  It's nice. Things are really cool.\n",
      "[04:03.640 --> 04:10.760]  Yeah. For example, I know I've been a year ago, I started thinking how to map those items for items.\n",
      "[04:10.760 --> 04:14.240]  And right now I'm trying to do it again.\n",
      "[04:14.240 --> 04:19.520]  And we are much more mature with like with beauty.\n",
      "[04:20.840 --> 04:22.040]  And we learn a lot.\n",
      "[04:22.040 --> 04:25.520]  And I know this strategy itself are more complete.\n",
      "[04:25.520 --> 04:34.560]  For example, I'm using multiple classification steps, even using models that I run locally on my laptop to test.\n",
      "[04:34.560 --> 04:37.520]  And it helps a lot to be able to do that.\n",
      "[04:38.480 --> 04:40.720]  So yeah, a lot's been going on.\n",
      "[04:40.720 --> 04:43.120]  Yeah, sounds like it.\n",
      "[04:43.120 --> 04:44.400]  But nice.\n",
      "[04:44.400 --> 04:48.360]  Yeah, that is very exciting and cool that you're getting to work with the kind of new experimental stuff.\n",
      "[04:48.360 --> 04:50.240]  And yeah, you mentioned a lot of transitions.\n",
      "[04:50.240 --> 04:53.600]  I know there's been some shaking up on the corporate team lately, right?\n",
      "[04:53.600 --> 04:55.600]  With like kind of people moving around and stuff like that.\n",
      "[04:55.600 --> 05:00.040]  Is that kind of related to the transitions you say of like products moving around?\n",
      "[05:00.040 --> 05:04.120]  Yeah, I used to have four clients.\n",
      "[05:04.640 --> 05:08.200]  I was responsible for the whole product.\n",
      "[05:08.200 --> 05:09.640]  So every month we met.\n",
      "[05:09.640 --> 05:11.880]  There were a lot of demands.\n",
      "[05:11.880 --> 05:14.280]  They asked questions, etc.\n",
      "[05:14.280 --> 05:20.320]  Half of them I already handed to the easier ones to deal with.\n",
      "[05:20.320 --> 05:22.560]  They're already with other teams.\n",
      "[05:22.560 --> 05:28.360]  But they're not a priority from a solution standpoint to migrate to other teams.\n",
      "[05:28.360 --> 05:32.680]  So not a huge, like not a lot of work.\n",
      "[05:32.760 --> 05:38.600]  But from time to time I just have to stop and pay attention to their needs,\n",
      "[05:38.600 --> 05:41.200]  which by itself is not that big of a thing.\n",
      "[05:41.200 --> 05:47.480]  But the fact that I have to stop kind of makes it harder to focus.\n",
      "[05:47.480 --> 05:51.960]  Yeah, but part of the that's how it is.\n",
      "[05:51.960 --> 05:54.520]  So yeah, makes sense.\n",
      "[05:54.520 --> 05:55.480]  Yeah, definitely makes sense.\n",
      "[05:55.480 --> 06:00.920]  I know Jordan is kind of coming in as like a new leader on the data side.\n",
      "[06:00.920 --> 06:02.120]  Yeah.\n",
      "[06:02.120 --> 06:05.400]  Have you gotten to work or talk with him much yet?\n",
      "[06:05.400 --> 06:11.080]  Yeah, we met once and then we spent like two weeks talking via email\n",
      "[06:11.080 --> 06:13.880]  because he wanted to understand a lot about beauty.\n",
      "[06:13.880 --> 06:16.760]  I think I got some good ideas.\n",
      "[06:16.760 --> 06:20.680]  I explained a lot of things to him.\n",
      "[06:20.680 --> 06:26.480]  Yeah, I really think that this change will be for the better.\n",
      "[06:26.480 --> 06:30.520]  But there's a lot of work for us to do.\n",
      "[06:30.520 --> 06:31.640]  I'm sure.\n",
      "[06:31.640 --> 06:34.560]  But I worked with him a bit back in the day when I was on the investor team\n",
      "[06:34.560 --> 06:38.280]  because he was like the kind of whole head of investor data.\n",
      "[06:38.280 --> 06:45.160]  Yeah, pretty definitely like a very, you know, very methodical, you know,\n",
      "[06:45.160 --> 06:50.880]  he was like the one who kind of I'd say was really responsible for YIPPID data\n",
      "[06:50.880 --> 06:56.800]  on the investor side, having its like standards of like data accuracy that it does.\n",
      "[06:56.800 --> 07:00.080]  He was very much the person who had a reputation of like, you know,\n",
      "[07:00.120 --> 07:05.040]  if you put out a certain report, he would like respond to it at like three in the morning,\n",
      "[07:05.040 --> 07:10.000]  being like, oh, like, what went into revising this one estimate here?\n",
      "[07:10.000 --> 07:12.680]  Or like, are you sure you calculated this one the right way?\n",
      "[07:12.680 --> 07:15.520]  And it was like, damn, we have like 50 different reports\n",
      "[07:15.520 --> 07:18.120]  and you're doing this to every one of them like all night.\n",
      "[07:18.120 --> 07:19.320]  Like, it's crazy stuff.\n",
      "[07:19.320 --> 07:22.960]  Definitely very, very smart, like brilliant person.\n",
      "[07:22.960 --> 07:27.040]  Like probably the biggest attention to detail than anyone I've ever worked with.\n",
      "[07:27.040 --> 07:34.600]  So I know some people can maybe find it a bit like intense, but I feel like he'll,\n",
      "[07:34.600 --> 07:39.600]  you know, I'm sure he'll bring a lot to the team, at least like, again,\n",
      "[07:39.600 --> 07:41.400]  like specifically on like the data side.\n",
      "[07:41.400 --> 07:44.800]  I just know corporate is also a lot different from investor because it's not like,\n",
      "[07:44.800 --> 07:49.760]  you know, we just have these like big methodologies that we're just kind of revising\n",
      "[07:49.760 --> 07:52.280]  and putting out reports on to many different clients, you know what I mean?\n",
      "[07:52.280 --> 07:57.240]  I know it's a lot more kind of custom for each client, a little more nuanced.\n",
      "[07:57.240 --> 08:00.080]  But anyways, yeah, I know he's joining.\n",
      "[08:00.080 --> 08:01.520]  I know Bill, right?\n",
      "[08:01.520 --> 08:04.120]  Bill Mensch is he's kind of joining the corporate team now, too.\n",
      "[08:04.120 --> 08:05.360]  Is that right?\n",
      "[08:05.360 --> 08:06.520]  I don't know.\n",
      "[08:07.800 --> 08:10.600]  I thought he I thought he was. I might be wrong.\n",
      "[08:10.600 --> 08:15.680]  Who else? And I just know there's many different things like Jimmy.\n",
      "[08:15.680 --> 08:17.960]  Is is he getting more involved again?\n",
      "[08:17.960 --> 08:19.280]  I know Jimmy used to be.\n",
      "[08:19.280 --> 08:21.920]  Yes, he used to be more.\n",
      "[08:21.920 --> 08:27.320]  I think now he'll be less involved since he's got a new role.\n",
      "[08:27.320 --> 08:28.400]  And he's like the president.\n",
      "[08:28.400 --> 08:32.400]  And now that now the Jordan came,\n",
      "[08:32.400 --> 08:37.640]  I think for a very short period of time, Jimmy was to be more involved.\n",
      "[08:37.640 --> 08:39.760]  And then things changed.\n",
      "[08:39.760 --> 08:41.440]  I know Will was also more involved, right?\n",
      "[08:41.440 --> 08:44.760]  And then Will kind of now I know Will now is very involved in like SpendHunt and Signal.\n",
      "[08:45.400 --> 08:50.440]  Yeah, well, it's been quite some time since I last spoke to him.\n",
      "[08:50.440 --> 08:52.920]  I think it was in April.\n",
      "[08:52.920 --> 08:56.040]  Yeah, yeah, I think he is less involved in corporate these days.\n",
      "[08:56.040 --> 08:59.200]  Exactly. A lot's been changing.\n",
      "[08:59.200 --> 09:06.120]  Yeah. And I have this feeling that there's a lot of opportunity\n",
      "[09:06.120 --> 09:08.600]  with very basic things.\n",
      "[09:08.600 --> 09:13.160]  Yeah, because we grew so fast, like things developed so fast.\n",
      "[09:13.160 --> 09:16.840]  And for example, there's a major pain point.\n",
      "[09:16.840 --> 09:19.320]  It's like QA in the data.\n",
      "[09:19.320 --> 09:25.840]  And it's just the main idea, the core concept of it is so simple.\n",
      "[09:25.840 --> 09:31.400]  Basically, what our analysts do is they plot a lot of charts,\n",
      "[09:31.400 --> 09:37.800]  comparing the data from last month and this month and say, hey, something seems weird here.\n",
      "[09:37.800 --> 09:41.680]  It changed. Very, very simple thing that could be automated.\n",
      "[09:41.680 --> 09:51.040]  But we could never do that due to the things that happened, the workload.\n",
      "[09:51.040 --> 10:00.320]  And I think it happens in a lot of things like communication and how we structure our workflows\n",
      "[10:00.320 --> 10:03.880]  and even how we deal with AI.\n",
      "[10:03.880 --> 10:07.000]  There's so many basic things that we haven't experienced yet.\n",
      "[10:07.960 --> 10:12.440]  I was talking about that with Paul today.\n",
      "[10:12.440 --> 10:15.000]  From my perspective.\n",
      "[10:15.000 --> 10:16.400]  From security?\n",
      "[10:16.400 --> 10:27.400]  Yes. From my perspective, the bottleneck more than ever is on what we will do with the tools\n",
      "[10:27.400 --> 10:30.240]  you already have rather than new tools coming in.\n",
      "[10:30.280 --> 10:39.560]  Because for example, GPT-5 just came out last week and it doesn't feel like a really big deal.\n",
      "[10:39.560 --> 10:47.560]  Because it's not really much about the tools, what we do with the data.\n",
      "[10:47.560 --> 10:54.040]  And we just haven't had a lot of time to experience, to try a lot of things.\n",
      "[10:54.040 --> 10:55.840]  Yeah.\n",
      "[10:55.840 --> 10:57.000]  That's it.\n",
      "[10:57.040 --> 10:58.720]  There's no time for us.\n",
      "[10:58.720 --> 11:06.040]  And it's like we're running after it, something that we need to experience.\n",
      "[11:06.040 --> 11:09.720]  And some things are going to go wrong and we just don't have time.\n",
      "[11:09.720 --> 11:15.800]  And the technology by itself runs much faster than our ability to make experiments.\n",
      "[11:15.800 --> 11:17.200]  Yeah.\n",
      "[11:17.200 --> 11:18.000]  I get that.\n",
      "[11:18.000 --> 11:22.040]  I mean, even in my work now, I've been using agents more than ever.\n",
      "[11:22.040 --> 11:25.320]  A lot of the work I was doing last week, I was working on a new feature and pretty much\n",
      "[11:25.320 --> 11:32.080]  did the whole thing in Cursor and really used GPT-5 for a good amount of it until I\n",
      "[11:32.080 --> 11:36.800]  ran out of the credit period or something and then I switched to Cloud.\n",
      "[11:36.800 --> 11:43.680]  But anyways, yeah, I feel like there are so many people who are still really learning\n",
      "[11:43.680 --> 11:45.400]  how to optimize these tools.\n",
      "[11:45.400 --> 11:50.360]  And I feel like there's so many new best practices that are continuing to come out that whether\n",
      "[11:50.360 --> 11:54.920]  it's on GPT-4 or GPT-5, it's still important learnings, right?\n",
      "[11:54.920 --> 12:01.480]  And I feel like at least my personal experiences, I haven't really had enough time to experiment\n",
      "[12:01.480 --> 12:05.080]  and learn these best practices and learn what works for me and works for the context of\n",
      "[12:05.080 --> 12:09.400]  my work because there's just so much work to get done.\n",
      "[12:09.400 --> 12:11.840]  And experimenting takes time, right?\n",
      "[12:11.840 --> 12:16.400]  It's not necessarily productive in the sense of like you might, for whatever task you're\n",
      "[12:16.400 --> 12:20.000]  working on at work, you might not get it done or you might not get it done as fast as if\n",
      "[12:20.000 --> 12:21.600]  you were just only working on it.\n",
      "[12:21.600 --> 12:24.000]  But there's a lot of valuable learning through the experimentation, right?\n",
      "[12:24.080 --> 12:27.000]  Because you're learning things to apply next time.\n",
      "[12:27.000 --> 12:29.080]  And I feel like there hasn't been a lot of opportunity for that.\n",
      "[12:29.080 --> 12:35.120]  And I feel like I see other people also working with these agents and everyone is slowly figuring\n",
      "[12:35.120 --> 12:36.120]  out what works for them.\n",
      "[12:36.120 --> 12:40.800]  And some people are a lot further along than others, you know, and a lot more advanced\n",
      "[12:40.800 --> 12:42.440]  in their use of these tools.\n",
      "[12:42.440 --> 12:49.000]  But I feel like I see people sharing things and discussions happening.\n",
      "[12:49.000 --> 12:53.480]  But I feel like at least I haven't had personally a lot of time to try out these different ideas\n",
      "[12:53.480 --> 12:57.280]  that people are coming up with or, yeah, like even just using the tools that we have\n",
      "[12:57.280 --> 12:58.280]  now, right?\n",
      "[12:58.280 --> 13:02.280]  Like even just using these chat bots or using something like cursor.\n",
      "[13:02.280 --> 13:05.960]  I feel like there's so many, even just like on the prompt side, right?\n",
      "[13:05.960 --> 13:09.600]  Like on the prompt engineering side, there's so many tricks people are coming up with and\n",
      "[13:09.600 --> 13:10.600]  different things.\n",
      "[13:10.600 --> 13:14.800]  And I feel like it's been really hard to just put a lot of time to experiment and just like\n",
      "[13:14.800 --> 13:19.560]  play around with these tools because then I wouldn't have necessarily enough time to\n",
      "[13:19.560 --> 13:20.960]  get everything done that we have to.\n",
      "[13:21.480 --> 13:27.440]  I feel like at least my feeling is that right now the company, you know, we're very much\n",
      "[13:27.440 --> 13:32.120]  in this like go, go, go phase where pretty much I feel like every single person has so\n",
      "[13:32.120 --> 13:35.560]  much they have to get done and deadlines are really tight right now.\n",
      "[13:35.560 --> 13:42.360]  And on top of it, there's a lot of transitions and migrations happening, you know, that we're\n",
      "[13:42.360 --> 13:47.200]  missing out on a lot of free time where I feel like just having free time where it's\n",
      "[13:47.280 --> 13:52.000]  like, oh, you don't have a deadline to hit today or, you know, you can spend a day just\n",
      "[13:52.000 --> 13:53.280]  playing around with something new.\n",
      "[13:53.280 --> 13:56.800]  Like, I feel like that's so valuable and there hasn't been too much opportunity for that\n",
      "[13:56.800 --> 14:00.640]  lately, at least at least in my words, maybe other people might disagree.\n",
      "[14:00.640 --> 14:05.080]  But that's how I think it's been for me and maybe some of the people I work closely with\n",
      "[14:05.080 --> 14:06.440]  how I'm kind of observing for them.\n",
      "[14:06.440 --> 14:08.000]  I can relate to that.\n",
      "[14:08.000 --> 14:14.120]  They're like a lot of small like ideas, things that would be nice to explore and keep piling\n",
      "[14:14.120 --> 14:17.960]  up because there's no time.\n",
      "[14:17.960 --> 14:21.600]  Yeah, spend a day exploring that idea and that's a day later that, you know, whatever\n",
      "[14:21.600 --> 14:24.240]  task you have to work on will get done.\n",
      "[14:24.240 --> 14:29.560]  And not only like the time itself on your calendar, but sometimes you just have to clean\n",
      "[14:29.560 --> 14:36.400]  your mind from a specific problem and allow yourself to think about other approaches and\n",
      "[14:36.400 --> 14:42.680]  something that I haven't been doing a lot these days.\n",
      "[14:42.680 --> 14:50.800]  So yeah, about the applications of AI or agents, I know from your applications, what\n",
      "[14:50.800 --> 14:57.160]  do you consider is the nicer thing that you do with AI?\n",
      "[14:57.160 --> 15:06.920]  Like the thing that you probably share with someone you just met to make conversation?\n",
      "[15:07.520 --> 15:12.400]  So this is very specific to the context of what I've been working on lately.\n",
      "[15:12.400 --> 15:15.200]  Well, there's been a few things actually.\n",
      "[15:15.200 --> 15:19.800]  One very simple answer I'd say is documentation.\n",
      "[15:19.800 --> 15:23.520]  With my older work on SpenHound, I was working with a lot of different APIs.\n",
      "[15:23.520 --> 15:27.560]  There's a lot of documentation on them, some were better than others.\n",
      "[15:27.560 --> 15:32.320]  I'd say ChatGPT has been great for if there's something hyper specific that I'm trying to\n",
      "[15:32.360 --> 15:36.800]  debug that there isn't really clear documentation on.\n",
      "[15:36.800 --> 15:40.680]  Previously, maybe a couple years ago, what I would do is end up on forms, right?\n",
      "[15:40.680 --> 15:45.480]  After Googling, I'd be on Stack Overflow or some other form for the API and trying to\n",
      "[15:45.480 --> 15:49.400]  find someone who's encountering the use case I am or the bug I am.\n",
      "[15:49.400 --> 15:52.840]  Now I feel like ChatGPT has actually been amazing at these types of things.\n",
      "[15:52.840 --> 15:56.160]  It just somehow knows the answers.\n",
      "[15:56.160 --> 16:00.480]  That's one very general thing, just like documentation, where rather than Googling and finding the\n",
      "[16:00.520 --> 16:04.600]  documentation and doing control F and looking for the thing you want to find, just asking\n",
      "[16:04.600 --> 16:08.160]  ChatGPT right away, I feel like I've been much better.\n",
      "[16:08.160 --> 16:13.760]  The other thing I was going to say is for me, this is much more specific to what I've\n",
      "[16:13.760 --> 16:17.760]  been working on, but basically I've joined this new role.\n",
      "[16:17.760 --> 16:23.560]  There's this whole new project and this whole new code base, basically a giant repository\n",
      "[16:23.560 --> 16:27.560]  of code that other people built, right, that I'm onboarding onto.\n",
      "[16:27.640 --> 16:31.240]  It's like the whole Atlas project for the signals application.\n",
      "[16:31.240 --> 16:35.600]  So it's a lot of new libraries I'm not used to.\n",
      "[16:35.600 --> 16:39.000]  The code is just organized in a way that I don't know because I didn't build it, right?\n",
      "[16:39.000 --> 16:40.000]  I'm just learning it.\n",
      "[16:40.000 --> 16:44.320]  So basically there's this whole code base that I could zoom in on one specific function\n",
      "[16:44.320 --> 16:48.240]  and I could look at it for 30 minutes an hour and figure out what it does, right?\n",
      "[16:48.240 --> 16:54.960]  But multiplying this by a thousand, I won't be able to efficiently learn everything that's\n",
      "[16:54.960 --> 16:56.560]  going on and how everything interacts.\n",
      "[16:56.560 --> 17:02.640]  So I'd say here using cursor has been really helpful because I have a task, I need to make\n",
      "[17:02.640 --> 17:07.440]  this chart do something else or I need something to change.\n",
      "[17:07.440 --> 17:10.360]  Maybe I don't know where in the code it happens.\n",
      "[17:10.360 --> 17:12.680]  I don't know maybe what I need to tweak to modify.\n",
      "[17:12.680 --> 17:19.600]  I'd say cursor and these AI agents have been incredible to both implement what I need to\n",
      "[17:19.600 --> 17:25.960]  do, so hey, I need to make this chart do this, do it, or propose the code changes to make\n",
      "[17:25.960 --> 17:27.720]  it happen.\n",
      "[17:27.720 --> 17:33.280]  But not only actually implement the change, but also teach me in the process.\n",
      "[17:33.280 --> 17:38.640]  So while it is suggesting, okay, you've got to modify this, this, and this, also being\n",
      "[17:38.640 --> 17:48.280]  like, okay, and ask it how do these interact with the other functions on this page or what\n",
      "[17:48.280 --> 17:50.760]  other files come into play here or you know what I mean?\n",
      "[17:51.520 --> 17:58.200]  Also having it almost like kind of create documentation for me on the fly has been really\n",
      "[17:58.200 --> 18:03.200]  helpful and questioning it a lot has been really helpful even if it's right, making\n",
      "[18:03.200 --> 18:05.720]  it doubt itself I find has been really good.\n",
      "[18:05.720 --> 18:09.360]  If I have a suspicion, oh, like in my head I was going to do it this other way, like\n",
      "[18:09.360 --> 18:12.400]  ask it that, like I thought I was going to do it this other way, like why is your way\n",
      "[18:12.400 --> 18:15.280]  better and have it kind of debate itself.\n",
      "[18:15.320 --> 18:22.600]  I feel like there have been many cases where it'll be wrong or it's very easy to influence\n",
      "[18:22.600 --> 18:27.520]  it, it'll by default if I suggest something else it'll say, oh yeah, of course you're\n",
      "[18:27.520 --> 18:32.040]  better because you're the human, you know, like you're right, I am wrong.\n",
      "[18:32.040 --> 18:34.920]  And that could not be the case, it could have been its suggestion was better, but just because\n",
      "[18:34.920 --> 18:39.400]  I'm pushing back on it, it might say, oh, my idea makes more sense.\n",
      "[18:39.400 --> 18:44.600]  It's definitely not perfect even with these latest models, like they clearly make mistakes\n",
      "[18:44.600 --> 18:49.440]  and they're very easy to influence, but I'd say just having it be like a tutor or like\n",
      "[18:49.440 --> 18:55.080]  a teacher for me, learning this whole new code base has been really helpful because\n",
      "[18:55.080 --> 18:58.000]  this is a whole application that was built in the past few months, there's no documentation\n",
      "[18:58.000 --> 19:02.280]  that exists, I'm pretty much just looking at a bunch of code, some of it formatted better\n",
      "[19:02.280 --> 19:08.160]  than others and I got to just start understanding it and work with it and I feel like had I\n",
      "[19:08.160 --> 19:12.920]  not had these AI agents, what I was able to do last week would have taken me over a month.\n",
      "[19:12.920 --> 19:19.560]  So just the productivity on learning something new, like a whole new application, a whole\n",
      "[19:19.560 --> 19:26.920]  new code base has been probably the biggest thing for me to answer your question, but\n",
      "[19:26.920 --> 19:31.640]  that is kind of more niche, like that's different than if you're building something yourself,\n",
      "[19:31.640 --> 19:35.160]  you know, this is for me kind of diving into something very large that someone else has\n",
      "[19:35.160 --> 19:39.360]  already built that I'm not familiar with.\n",
      "[19:39.360 --> 19:40.360]  That's a great application.\n",
      "[19:40.360 --> 19:41.360]  Yeah.\n",
      "[19:41.360 --> 19:42.360]  Yeah.\n",
      "[19:42.800 --> 19:47.040]  That first use case is much more the traditional one where it's just kind of treating it as\n",
      "[19:47.040 --> 19:48.520]  like Google, you know?\n",
      "[19:48.520 --> 19:51.760]  Oh, like I'm getting this bug, how do I fix it?\n",
      "[19:51.760 --> 19:54.040]  Learning documentation that I can't find online.\n",
      "[19:54.040 --> 19:57.280]  That has been great too, but that is much more where I feel like, you know, what many\n",
      "[19:57.280 --> 19:59.920]  people have just been using these chatbots for, right, is like just treating it like\n",
      "[19:59.920 --> 20:04.280]  a Google, like a search query, just finding information easier than having to navigate,\n",
      "[20:04.280 --> 20:09.200]  you know, a browser, but yeah, how about you?\n",
      "[20:09.200 --> 20:12.960]  I know we're about to hit, do you have to leave on another meeting right now?\n",
      "[20:12.960 --> 20:16.120]  No, just focus time until the end of the day.\n",
      "[20:16.120 --> 20:18.320]  Okay, well, let's just stay a few more minutes.\n",
      "[20:18.320 --> 20:20.480]  I want to hear your answer to that same question.\n",
      "[20:20.480 --> 20:26.960]  Okay, I think I've been experimenting a lot and especially one thing that surprised me\n",
      "[20:26.960 --> 20:35.680]  is how useful I found to be like running models locally on my laptop.\n",
      "[20:35.680 --> 20:45.800]  Like Luma, like 3.1.1.8 bit of parameters, like helps a lot to experiment, but still\n",
      "[20:45.800 --> 20:50.960]  I think the most interesting thing I've been doing is simply connect cursor with my notes\n",
      "[20:50.960 --> 20:53.840]  app because I put everything there.\n",
      "[20:53.840 --> 21:01.440]  Sometimes I even like record, if there's a meeting that was recorded, I just made like\n",
      "[21:01.440 --> 21:04.520]  some, I summarized it, added to my notes.\n",
      "[21:05.200 --> 21:11.560]  Yeah, I just put everything there because the workflow works for me and I think it's\n",
      "[21:11.560 --> 21:15.560]  a nicer thing because sometimes I just want to remember what did I do for this specific\n",
      "[21:15.560 --> 21:22.000]  client over the past three months and it's simple and it integrates with my daily life.\n",
      "[21:22.000 --> 21:30.040]  So it's almost like my, people used to create strategies to use Obsidian for like, to be\n",
      "[21:30.040 --> 21:37.240]  like to work somehow similar to a second brain, but you couldn't do that because you\n",
      "[21:37.240 --> 21:45.560]  had the links, have the knowledge, but you couldn't like query it or understand it in\n",
      "[21:45.560 --> 21:52.600]  a broader, like holistic way and you kind of can't.\n",
      "[21:52.600 --> 21:58.880]  So let me show you what my notes look like.\n",
      "[21:58.880 --> 22:03.640]  Those are my notes and that's excluding all the daily notes and for every day I have this\n",
      "[22:03.640 --> 22:12.280]  specific template that keeps appearing for every day and it simply makes me like comfortable\n",
      "[22:12.280 --> 22:17.280]  to just keep like putting my thoughts there.\n",
      "[22:17.280 --> 22:22.840]  For example, I just finished Logged the Rings, the Fellowship of the Ring and here's like\n",
      "[22:22.840 --> 22:26.240]  all my quotes that I want to remember.\n",
      "[22:26.280 --> 22:33.200]  And then I export a PDF and that's a resource for myself in the future.\n",
      "[22:33.200 --> 22:40.480]  I think that's great, you know, it's like structuring my own knowledge base.\n",
      "[22:40.480 --> 22:46.520]  Then now I can query and now I can, for example, in the future, it wasn't like this when I\n",
      "[22:46.520 --> 22:52.760]  first wrote because I was reading, but then I just asked her, so to help me like structure\n",
      "[22:52.760 --> 22:53.760]  everything.\n",
      "[22:53.760 --> 22:58.480]  So of course I had to, I used some other like tools to make sure that those references were\n",
      "[22:58.480 --> 22:59.480]  correct.\n",
      "[22:59.480 --> 23:00.480]  Yeah.\n",
      "[23:00.480 --> 23:06.000]  I was like, I'm comfortable doing it because if I need in the future, I can just remodel\n",
      "[23:06.000 --> 23:09.120]  it because the course, the model can do it.\n",
      "[23:09.120 --> 23:16.600]  So I only care about like thinking and writing in a way that it works for me.\n",
      "[23:16.600 --> 23:18.760]  Yeah, that's so cool.\n",
      "[23:18.760 --> 23:20.560]  Also these are really great clips you got here.\n",
      "[23:20.560 --> 23:21.880]  I love that book.\n",
      "[23:21.880 --> 23:26.840]  Oh yeah, I read it for the first time.\n",
      "[23:26.840 --> 23:27.840]  Oh nice.\n",
      "[23:27.840 --> 23:33.480]  I've been meaning, I've kind of over the past couple years solely been rereading the trilogy.\n",
      "[23:33.480 --> 23:34.480]  I did the first two.\n",
      "[23:34.480 --> 23:39.800]  I read Fellowship, took a break, read some other books, read Two Towers, took a break.\n",
      "[23:39.800 --> 23:43.120]  Now I need to start Return of the King at some point.\n",
      "[23:43.120 --> 23:45.200]  Yeah, I don't know.\n",
      "[23:45.200 --> 23:52.520]  I read, I had already read The Hobbit like three times and I thought that was like a\n",
      "[23:52.520 --> 23:53.520]  natural progression.\n",
      "[23:53.520 --> 23:54.520]  It's great.\n",
      "[23:54.520 --> 23:55.520]  It's so beautiful.\n",
      "[23:55.520 --> 24:00.840]  And I know I probably just have, I'll probably just keep reading it, but I'd say like I'll\n",
      "[24:00.840 --> 24:06.080]  keep like two books, one technical book or more like self-development, not like fictional\n",
      "[24:06.080 --> 24:09.320]  book and a book at the same time like always.\n",
      "[24:09.320 --> 24:10.880]  Yeah, yeah, these are great.\n",
      "[24:10.880 --> 24:13.440]  I mean they get definitely, I think the books are beautiful.\n",
      "[24:13.440 --> 24:18.080]  They even can be a little bit dense, like Two Towers was quite, it's a bit more dense\n",
      "[24:18.080 --> 24:19.080]  even.\n",
      "[24:19.080 --> 24:20.080]  Okay.\n",
      "[24:20.080 --> 24:24.200]  But, you know, Tolkien definitely spent a lot of time on the detail, right, imagery\n",
      "[24:24.200 --> 24:25.200]  and things like that.\n",
      "[24:25.200 --> 24:26.200]  Yeah.\n",
      "[24:26.200 --> 24:28.200]  Rather than moving along a lot, you know.\n",
      "[24:28.200 --> 24:32.720]  I simply have to learn to read, so much easier task.\n",
      "[24:32.720 --> 24:33.720]  That is great.\n",
      "[24:33.720 --> 24:34.720]  It is great.\n",
      "[24:34.720 --> 24:38.480]  But anyways, that is so cool.\n",
      "[24:38.480 --> 24:41.080]  That's awesome.\n",
      "[24:41.560 --> 24:45.960]  I know you're definitely like a power user for Obsidian.\n",
      "[24:45.960 --> 24:46.960]  Yeah.\n",
      "[24:46.960 --> 24:52.440]  And that's something that I want to share because I saw no one that does that, like\n",
      "[24:52.440 --> 25:00.000]  on the internet, not a single like productivity guru or I think as you mature, as you grow,\n",
      "[25:00.000 --> 25:04.920]  you start doing things that only you do or understanding things that only, the way that\n",
      "[25:04.920 --> 25:05.920]  only you understand.\n",
      "[25:05.920 --> 25:09.920]  So it's valuable for the world that you share those things.\n",
      "[25:09.920 --> 25:10.920]  Yeah.\n",
      "[25:10.920 --> 25:14.200]  I know you mentioned like doing like a YouTube videos, like is this going to be a video you\n",
      "[25:14.200 --> 25:15.200]  make?\n",
      "[25:15.200 --> 25:16.200]  Probably.\n",
      "[25:16.200 --> 25:17.200]  Yeah.\n",
      "[25:17.200 --> 25:20.880]  Because I have like a really big backlog of things, for example.\n",
      "[25:20.880 --> 25:21.880]  Yeah.\n",
      "[25:21.880 --> 25:22.880]  Small things.\n",
      "[25:22.880 --> 25:29.120]  At the start of my career or even today, I sometimes just want to develop something locally\n",
      "[25:29.120 --> 25:31.440]  on PySpark.\n",
      "[25:31.440 --> 25:33.600]  And there's no super easy way.\n",
      "[25:33.600 --> 25:38.240]  I never understood how to like install it locally.\n",
      "[25:38.240 --> 25:41.080]  But now I do understand and it's so simple.\n",
      "[25:41.080 --> 25:42.480]  And that's something that I could do.\n",
      "[25:42.480 --> 25:47.640]  And I do that like on my, on a Docker container.\n",
      "[25:47.640 --> 25:49.040]  I set up a Docker container.\n",
      "[25:49.040 --> 25:53.840]  I connect like cursor to it and I run everything there.\n",
      "[25:53.840 --> 25:58.840]  And it took, because it took me some time to understand that.\n",
      "[25:58.840 --> 26:00.560]  And now it seems totally clear.\n",
      "[26:00.560 --> 26:07.080]  I could express the way I understand things to the world and someone maybe like, maybe\n",
      "[26:07.760 --> 26:14.080]  helped by it because it's he or she's going to understand things because I did that way.\n",
      "[26:14.080 --> 26:21.920]  I feel like, you know, we are not that unique in that if you're running into a challenge\n",
      "[26:21.920 --> 26:27.800]  or you don't understand something and you need help, odds are someone else is going\n",
      "[26:27.800 --> 26:30.160]  to find themselves in that same situation where they don't understand it and might need\n",
      "[26:30.160 --> 26:31.160]  help too, you know?\n",
      "[26:31.160 --> 26:34.760]  So I feel like if you ever have something like that where it's like you couldn't, you\n",
      "[26:35.360 --> 26:41.560]  know, maybe easily find like a resource to help out or you were able to figure it out\n",
      "[26:41.560 --> 26:45.880]  on your own, like that knowledge probably will be useful for somebody else too, right?\n",
      "[26:45.880 --> 26:46.880]  Because like.\n",
      "[26:46.880 --> 26:47.880]  Exactly.\n",
      "[26:47.880 --> 26:52.680]  But at the same time, we're, I think I'm finding the things that people from Stack Overflow\n",
      "[26:52.680 --> 26:59.040]  like discovered a long time ago is that the knowledge in the world is not that extensive\n",
      "[26:59.040 --> 27:00.520]  that you can't contribute.\n",
      "[27:00.520 --> 27:01.520]  Yeah.\n",
      "[27:01.520 --> 27:02.520]  100%.\n",
      "[27:02.520 --> 27:05.000]  There's always new things, there are better ways to do things.\n",
      "[27:05.000 --> 27:13.400]  Yeah, there's like 20 videos on a specific topic on YouTube and there's maybe space for\n",
      "[27:13.400 --> 27:16.760]  like one more that sees things a little bit differently.\n",
      "[27:16.760 --> 27:18.800]  Especially with new ideas.\n",
      "[27:18.800 --> 27:19.800]  Amazing.\n",
      "[27:19.800 --> 27:22.360]  I do have to jump now, but great catching up today.\n",
      "[27:22.360 --> 27:23.360]  Oh, it was great.\n",
      "[27:23.360 --> 27:25.720]  And thank you a lot for your extra eight minutes.\n",
      "[27:25.720 --> 27:27.440]  Oh yeah, no, of course.\n",
      "[27:27.440 --> 27:28.680]  I'm in focus time now too.\n",
      "[27:28.680 --> 27:29.680]  So no, no.\n",
      "[27:29.680 --> 27:30.680]  All right.\n",
      "[27:30.680 --> 27:31.680]  Thank you very much.\n",
      "[27:31.840 --> 27:32.840]  Great rest of your week.\n",
      "[27:32.840 --> 27:33.840]  Thanks.\n",
      "[27:33.840 --> 27:34.840]  You too.\n",
      "[27:34.840 --> 27:35.840]  Talk to you later.\n",
      "[27:35.840 --> 27:36.840]  See you.\n",
      "[27:36.840 --> 27:37.840]  Bye.\n",
      "[27:37.840 --> 27:38.840]  Okay.\n",
      "[27:38.840 --> 27:39.840]  Puta merda, hein, Marcio?\n",
      "[27:39.840 --> 27:40.840]  Eu adoro isso.\n",
      "[28:41.000 --> 28:43.000]  Deixa eu ver o que eu vou fazer agora.\n",
      "[28:55.000 --> 28:57.000]  Vou ver, já estou com um clínic...\n",
      "[29:01.000 --> 29:02.000]  ...Coslex.\n",
      "[29:07.000 --> 29:08.000]  Oh, interesting.\n",
      "[29:11.000 --> 29:12.000]  Yeah.\n",
      "[29:36.000 --> 29:37.000]  It's a good village.\n",
      "[29:41.000 --> 29:42.000]  Okay.\n",
      "[29:42.000 --> 29:43.000]  Interesting.\n",
      "[30:11.000 --> 30:14.000]  Um...\n",
      "[30:20.000 --> 30:21.000]  More things.\n",
      "[30:29.000 --> 30:30.000]  Let me know if...\n",
      "[30:36.000 --> 30:39.000]  I can log into...\n",
      "[30:40.840 --> 30:42.000]  Okay.\n",
      "[31:10.840 --> 31:12.000]  Okay.\n",
      "[31:40.840 --> 31:42.000]  Okay.\n",
      "[32:10.840 --> 32:12.000]  Okay.\n",
      "[32:41.000 --> 32:42.000]  Uh...\n",
      "[32:50.000 --> 32:51.000]  Oh.\n",
      "[32:51.000 --> 32:53.000]  Deixa eu ver isso aqui.\n",
      "[32:58.000 --> 32:59.000]  Vamos ver uma coisa.\n",
      "[33:10.840 --> 33:12.000]  Yeah.\n",
      "[33:40.840 --> 33:42.000]  Yeah.\n",
      "[34:10.840 --> 34:12.000]  Yeah.\n",
      "[34:40.840 --> 34:42.000]  Um...\n",
      "[35:10.840 --> 35:12.000]  Um...\n",
      "[35:40.840 --> 35:42.000]  Um...\n",
      "[36:11.000 --> 36:12.000]  Um...\n",
      "[36:22.000 --> 36:23.000]  Um...\n",
      "[36:25.000 --> 36:26.000]  Okay.\n",
      "[36:33.000 --> 36:34.000]  Um...\n",
      "[36:34.000 --> 36:35.000]  Okay.\n",
      "[36:35.160 --> 36:36.160]  Um...\n",
      "[36:53.160 --> 36:54.160]  Okay.\n",
      "[37:05.160 --> 37:06.160]  Um...\n",
      "[37:20.160 --> 37:21.160]  Um...\n",
      "[37:35.160 --> 37:36.160]  Um...\n",
      "[37:43.160 --> 37:44.160]  Um...\n",
      "[37:51.160 --> 37:52.160]  Um...\n",
      "[37:59.160 --> 38:00.160]  Um...\n",
      "[38:05.160 --> 38:06.160]  Um...\n",
      "[38:20.160 --> 38:21.160]  Aí eu coloquei livros.\n",
      "[38:23.160 --> 38:25.160]  Que legal!\n",
      "[38:25.160 --> 38:30.160]  E essa luz aqui faz milagres porque fica aquela aura atrás de você que equilibra a luz.\n",
      "[38:30.160 --> 38:31.160]  Hein?\n",
      "[38:31.160 --> 38:32.160]  Bonita, viu?\n",
      "[38:32.160 --> 38:33.160]  Hein?\n",
      "[38:33.160 --> 38:34.160]  E o MDF é quase igual.\n",
      "[38:35.160 --> 38:38.160]  Eu não tinha reparado nisso.\n",
      "[38:38.160 --> 38:39.160]  Gostei.\n",
      "[38:39.160 --> 38:40.160]  E é isso.\n",
      "[38:40.160 --> 38:41.160]  Eu fiquei feliz.\n",
      "[38:41.160 --> 38:42.160]  É, bonita?\n",
      "[38:42.160 --> 38:43.160]  Gostei.\n",
      "[38:43.160 --> 38:47.160]  Eu posso fazer livros, esconder minha bolsa, mas é muita coisa.\n",
      "[38:47.160 --> 38:50.160]  Olha quantas coisas eu tenho pra fazer que eu não vou fazer.\n",
      "[38:50.160 --> 38:54.160]  Caramba, tá muito legal que eu não vou fazer.\n",
      "[38:54.160 --> 38:55.160]  É, não vai dar?\n",
      "[38:55.160 --> 38:56.160]  Ah, realmente.\n",
      "[38:56.160 --> 38:57.160]  Isso aqui é para o teu podcast.\n",
      "[38:57.160 --> 38:58.160]  Boa tarde.\n",
      "[38:58.160 --> 38:59.160]  Ah, e eu não te compro...\n",
      "[38:59.160 --> 39:00.160]  Eu queria falar com a Guilherme.\n",
      "[39:00.160 --> 39:01.160]  Eu agora tenho um podcast.\n",
      "[39:01.320 --> 39:04.320]  Opa, peraí.\n",
      "[39:04.320 --> 39:09.320]  Vou começar meu canal no YouTube.\n",
      "[39:09.320 --> 39:10.320]  Não acredita.\n",
      "[39:10.320 --> 39:11.320]  Valendo.\n",
      "[39:11.320 --> 39:26.320]  Então, se você vier em YouTube e procurar exatamente para esse cara, você vai ver\n",
      "[39:26.320 --> 39:27.320]  um canal.\n",
      "[39:27.320 --> 39:28.320]  Ah, vamos escrever.\n",
      "[39:28.320 --> 39:29.320]  Mas eu não tenho nada ainda.\n",
      "[39:29.480 --> 39:32.480]  Eu falei com um cara que não tá nem aparecendo aqui.\n",
      "[39:32.480 --> 39:33.480]  O que?\n",
      "[39:33.480 --> 39:34.480]  O que é isso, meu Deus?\n",
      "[39:34.480 --> 39:35.480]  Isso aqui é o meu canal do FC?\n",
      "[39:35.480 --> 39:38.480]  Enfim, se você procurar aqui, vai ter meu canal, vai surgir.\n",
      "[39:38.480 --> 39:41.480]  Então eu coloco Guilherme, The Day Again.\n",
      "[39:41.480 --> 39:44.480]  No momento que eu tiver um vídeo, eu vou compartilhar.\n",
      "[39:44.480 --> 39:45.480]  Tá, compartilha.\n",
      "[39:45.480 --> 39:46.480]  É esse aqui.\n",
      "[39:46.480 --> 39:47.480]  Eu vou deixar o link aqui.\n",
      "[39:47.480 --> 39:48.480]  Você pode fazer isso.\n",
      "[39:48.480 --> 39:49.480]  Você pode fazer isso.\n",
      "[39:49.480 --> 39:50.480]  Você pode fazer isso.\n",
      "[39:50.480 --> 39:51.480]  Você pode fazer isso.\n",
      "[39:51.480 --> 39:52.480]  Você pode fazer isso.\n",
      "[39:52.480 --> 39:53.480]  Você pode fazer isso.\n",
      "[39:53.480 --> 39:54.480]  Você pode fazer isso.\n",
      "[39:54.480 --> 39:55.480]  Você pode fazer isso.\n",
      "[39:55.480 --> 39:56.480]  Você pode fazer isso.\n",
      "[39:56.480 --> 39:57.480]  Você pode fazer isso.\n",
      "[39:57.480 --> 39:58.480]  Você pode fazer isso.\n",
      "[39:58.640 --> 40:01.640]  É esse perfil aqui ou não?\n",
      "[40:01.640 --> 40:02.640]  Não, não.\n",
      "[40:02.640 --> 40:03.640]  Isso aqui é...\n",
      "[40:03.640 --> 40:04.640]  Não.\n",
      "[40:04.640 --> 40:05.640]  Não, não.\n",
      "[40:05.640 --> 40:08.640]  Isso aqui é a conta da FC.\n",
      "[40:08.640 --> 40:09.640]  Entendi.\n",
      "[40:09.640 --> 40:12.640]  Aí eu vou publicar.\n",
      "[40:12.640 --> 40:15.640]  Vai começar porque eu vou falar com o cara que já é produtor de eventos, que trabalha\n",
      "[40:15.640 --> 40:16.640]  com essas coisas.\n",
      "[40:16.640 --> 40:19.640]  Eu mandei um vídeo que eu tinha feito pra ele e ele falou que antes ficou muito legal.\n",
      "[40:19.640 --> 40:22.640]  Aí eu fiquei animado e vou começar a escrever e fazer coisas.\n",
      "[40:22.640 --> 40:25.640]  Que legal, que chique.\n",
      "[40:25.640 --> 40:26.640]  O espaço tá me ajudando.\n",
      "[40:26.640 --> 40:27.640]  Eu tô achando lindo, maravilhoso.\n",
      "[40:27.800 --> 40:28.800]  Eu tô achando muito perfeito.\n",
      "[40:28.800 --> 40:29.800]  Ficou perfeito pro espaço.\n",
      "[40:29.800 --> 40:30.800]  Ai, que botão.\n",
      "[40:30.800 --> 40:31.800]  Tá com vermelho.\n",
      "[40:31.800 --> 40:34.800]  Vai que eu boto aqui e você fala que tá feio, mulher.\n",
      "[40:34.800 --> 40:37.800]  E o MDF é muito parecido com o MDF da mesa.\n",
      "[40:37.800 --> 40:38.800]  Muito parecido.\n",
      "[40:38.800 --> 40:39.800]  Que feliz.\n",
      "[40:39.800 --> 40:40.800]  A mesa não, né?\n",
      "[40:40.800 --> 40:41.800]  Acaba é muita coisa.\n",
      "[40:41.800 --> 40:42.800]  Chique.\n",
      "[40:42.800 --> 40:47.800]  Tô tentando configurar pra quem manda a senheza pra mim.\n",
      "[40:47.800 --> 40:50.800]  Era só isso que eu queria compartilhar.\n",
      "[40:50.800 --> 40:51.800]  É massa, viu?\n",
      "[40:51.800 --> 40:54.800]  Tô cansada, mas estou empolgada.\n",
      "[40:54.800 --> 40:55.800]  Que bom.\n",
      "[40:55.960 --> 40:56.960]  Que bom.\n",
      "[40:56.960 --> 40:59.960]  A senheza é uma bela, não sei se ela parece, mas quem me lembra.\n",
      "[40:59.960 --> 41:00.960]  Isso, pra dar uma corte.\n",
      "[41:00.960 --> 41:01.960]  Chique, chique.\n",
      "[41:01.960 --> 41:10.960]  Eu vou se precisar de tirar o carro pra deixar a vida do time legal ou não.\n",
      "[41:10.960 --> 41:13.960]  Ah não, eu vou pegar meu coisa, vou me arrumar, voltar aí pra trabalhar.\n",
      "[41:13.960 --> 41:14.960]  Tá.\n",
      "[41:14.960 --> 41:15.960]  Tem uma coisa pra fazer.\n",
      "[41:15.960 --> 41:16.960]  Tá bom.\n",
      "[01:08:47.200 --> 01:09:06.200]  boring\n",
      "[01:09:06.200 --> 01:09:28.200]  there we have jet, or flow, in thedessus there we have rocket, okay?\n",
      "[01:11:58.200 --> 01:11:59.200]  Okay.\n",
      "[01:12:28.200 --> 01:12:29.200]  Okay.\n",
      "[01:12:58.200 --> 01:12:59.200]  Okay.\n",
      "[01:13:28.200 --> 01:13:29.200]  Okay.\n",
      "[01:13:58.200 --> 01:13:59.200]  Okay.\n",
      "[01:14:28.200 --> 01:14:29.200]  Okay.\n",
      "[01:14:58.200 --> 01:14:59.200]  Okay.\n",
      "[01:15:28.200 --> 01:15:29.200]  Okay.\n",
      "[01:15:58.200 --> 01:15:59.200]  Okay.\n",
      "[01:16:28.200 --> 01:16:29.200]  Okay.\n",
      "[01:16:58.200 --> 01:16:59.200]  Okay.\n",
      "[01:17:28.200 --> 01:17:29.200]  Okay.\n",
      "[01:17:58.200 --> 01:17:59.200]  Okay.\n",
      "[01:18:28.200 --> 01:18:29.200]  Okay.\n",
      "[01:18:58.200 --> 01:18:59.200]  Okay.\n",
      "[01:19:28.200 --> 01:19:29.200]  Okay.\n",
      "[01:19:58.200 --> 01:19:59.200]  Okay.\n",
      "[01:20:28.200 --> 01:20:29.200]  Okay.\n",
      "[01:20:58.200 --> 01:20:59.200]  Okay.\n",
      "[01:21:28.200 --> 01:21:29.200]  Okay.\n",
      "[01:21:58.200 --> 01:21:59.200]  Okay.\n",
      "[01:22:28.200 --> 01:22:29.200]  Okay.\n",
      "[01:22:58.200 --> 01:22:59.200]  Okay.\n",
      "[01:23:28.200 --> 01:23:29.200]  Okay.\n",
      "[01:23:58.200 --> 01:23:59.200]  Okay.\n",
      "[01:24:28.200 --> 01:24:30.200]  Oh, you fool.\n",
      "[01:24:58.200 --> 01:24:59.200]  Okay.\n",
      "[01:25:28.200 --> 01:25:29.200]  Okay.\n",
      "[01:25:58.200 --> 01:25:59.200]  Okay.\n",
      "[01:26:28.200 --> 01:26:29.200]  Okay.\n",
      "[01:26:58.200 --> 01:26:59.200]  Okay.\n",
      "[01:27:28.200 --> 01:27:29.200]  Okay.\n",
      "[01:27:58.200 --> 01:27:59.200]  Okay.\n",
      "[01:28:28.200 --> 01:28:29.200]  Okay.\n",
      "[01:28:58.200 --> 01:28:59.200]  Okay.\n",
      "[01:29:28.200 --> 01:29:29.200]  Okay.\n",
      "[01:29:58.200 --> 01:29:59.200]  Okay.\n",
      "[01:30:28.200 --> 01:30:29.200]  Okay.\n",
      "[01:30:58.200 --> 01:30:59.200]  Okay.\n",
      "[01:31:28.200 --> 01:31:29.200]  Okay.\n",
      "[01:31:58.200 --> 01:31:59.200]  Okay.\n",
      "[01:32:28.200 --> 01:32:29.200]  Okay.\n",
      "[01:32:58.200 --> 01:32:59.200]  Okay.\n",
      "[01:33:28.200 --> 01:33:29.200]  Okay.\n",
      "[01:33:58.200 --> 01:33:59.200]  Okay.\n",
      "[01:34:28.200 --> 01:34:29.200]  Okay.\n",
      "[01:34:58.200 --> 01:34:59.200]  Okay.\n",
      "[01:35:28.200 --> 01:35:29.200]  Okay.\n",
      "[01:35:58.200 --> 01:35:59.200]  Okay.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 3. Transcribe the audio\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#    This is where the main processing happens. It might take a while.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting transcription of \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Set verbose=True to see progress\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTranscription complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 4. Access and print the transcribed text\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/whisper/transcribe.py:295\u001b[39m, in \u001b[36mtranscribe\u001b[39m\u001b[34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    293\u001b[39m     decode_options[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m] = all_tokens[prompt_reset_since:]\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m result: DecodingResult = \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m tokens = torch.tensor(result.tokens)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/whisper/transcribe.py:201\u001b[39m, in \u001b[36mtranscribe.<locals>.decode_with_fallback\u001b[39m\u001b[34m(segment)\u001b[39m\n\u001b[32m    198\u001b[39m     kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    200\u001b[39m options = DecodingOptions(**kwargs, temperature=t)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m decode_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m needs_fallback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    205\u001b[39m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    206\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m decode_result.compression_ratio > compression_ratio_threshold\n\u001b[32m    207\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/whisper/decoding.py:824\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, mel, options, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    822\u001b[39m     options = replace(options, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m result = \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/whisper/decoding.py:718\u001b[39m, in \u001b[36mDecodingTask.run\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    715\u001b[39m tokenizer: Tokenizer = \u001b[38;5;28mself\u001b[39m.tokenizer\n\u001b[32m    716\u001b[39m n_audio: \u001b[38;5;28mint\u001b[39m = mel.shape[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m audio_features: Tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_audio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# encoder forward pass\u001b[39;00m\n\u001b[32m    719\u001b[39m tokens: Tensor = torch.tensor([\u001b[38;5;28mself\u001b[39m.initial_tokens]).repeat(n_audio, \u001b[32m1\u001b[39m)\n\u001b[32m    721\u001b[39m \u001b[38;5;66;03m# detect language if requested, overwriting the language token\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/whisper/decoding.py:655\u001b[39m, in \u001b[36mDecodingTask._get_audio_features\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    653\u001b[39m     audio_features = mel\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     audio_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio_features.dtype != (\n\u001b[32m    658\u001b[39m     torch.float16 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.options.fp16 \u001b[38;5;28;01melse\u001b[39;00m torch.float32\n\u001b[32m    659\u001b[39m ):\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    661\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maudio_features has an incorrect dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_features.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    662\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/whisper/model.py:201\u001b[39m, in \u001b[36mAudioEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    198\u001b[39m x = (x + \u001b[38;5;28mself\u001b[39m.positional_embedding).to(x.dtype)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_post(x)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/whisper/model.py:170\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cross_attn:\n\u001b[32m    169\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.cross_attn(\u001b[38;5;28mself\u001b[39m.cross_attn_ln(x), xa, kv_cache=kv_cache)[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/GuilhermeTheDataGuy/speech_to_text/venv/lib/python3.12/site-packages/whisper/model.py:46\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# 1. Load the Whisper model\n",
    "#    Model options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "#    \"medium\" is a good balance of speed and accuracy.\n",
    "#    The model is downloaded automatically the first time you run it.\n",
    "print(\"Loading Whisper model...\")\n",
    "model = whisper.load_model(\"medium\")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# 2. Specify the path to your video or audio file\n",
    "video_file = \"ben_mentoring_august.mov\"  # Replace with your video file name\n",
    "\n",
    "# 3. Transcribe the audio\n",
    "#    This is where the main processing happens. It might take a while.\n",
    "print(f\"Starting transcription of '{video_file}'...\")\n",
    "result = model.transcribe(video_file, verbose=True) # Set verbose=True to see progress\n",
    "print(\"Transcription complete.\")\n",
    "\n",
    "\n",
    "# 4. Access and print the transcribed text\n",
    "transcribed_text = result[\"text\"]\n",
    "print(\"\\n--- Transcription ---\")\n",
    "print(transcribed_text)\n",
    "print(\"---------------------\\n\")\n",
    "\n",
    "\n",
    "# 5. Saving the transcription to a text file\n",
    "output_filename = \"audio2.txt\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(transcribed_text)\n",
    "\n",
    "print(f\"Transcription saved to '{output_filename}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
